---
title: Food Ingredients Detection API
emoji: üçΩÔ∏è
colorFrom: blue
colorTo: green
sdk: docker
app_port: 7860
---

# Food Ingredients Detection API

API FastAPI para detectar **ingredientes visibles en fotos de platos** usando **Grounding DINO** (modelo vision-language, zero-shot). No usa Roboflow ni modelos entrenados en datasets cerrados.

## Modelo

- **Grounding DINO** (Hugging Face): detecci√≥n guiada por texto (zero-shot).
- Modelo por defecto: `IDEA-Research/grounding-dino-tiny`.
- La primera petici√≥n a `/detect` descargar√° el modelo (se ejecuta localmente).

### Variables de entorno (opcionales)

| Variable | Descripci√≥n | Default |
|----------|-------------|---------|
| `GROUNDING_DINO_MODEL_ID` | ID del modelo en Hugging Face | `IDEA-Research/grounding-dino-tiny` |
| `GROUNDING_DINO_BOX_THRESHOLD` | Umbral de confianza del bounding box (0‚Äì1) | `0.30` |
| `GROUNDING_DINO_TEXT_THRESHOLD` | Umbral de alineaci√≥n texto-imagen (0‚Äì1) | `0.25` |

## Endpoints

| M√©todo | Ruta | Descripci√≥n |
|--------|------|-------------|
| GET | `/` | Info de la API y enlaces |
| GET | `/health` | Health check |
| GET | `/docs` | Documentaci√≥n Swagger UI |
| POST | `/detect` | Sube imagen ‚Üí JSON con ingredientes (label, score, opcional box) |
| POST | `/detect/image` | Sube imagen ‚Üí imagen con cajas y etiquetas dibujadas (JPEG) |
| POST | `/corrections` | MLOps: guarda correcci√≥n human-in-the-loop (imagen + detected + corrected + consent) |

### Par√°metros de POST /detect

- **category** (opcional): `breakfast`, `lunch`, `snack`, `dinner` ‚Äî filtra ingredientes por categor√≠a.
- **ingredients_prompt** (opcional): ingredientes separados por comas (ej: `rice, lentils, tomato`). Si no se env√≠a, se usa la lista base.
- **box_threshold**, **text_threshold** (opcional): umbrales del modelo (0‚Äì1).
- **include_boxes** (default `true`): incluir coordenadas de las cajas en la respuesta.

Formatos de imagen: JPEG, PNG, WebP, BMP.

## Uso local

```bash
cd nutri-ai-backend
pip install -r requirements.txt
uvicorn main:app --reload --port 8000
```

Abre **http://localhost:8000/docs** y prueba **POST /detect** con una foto de un plato.

### Probar con curl

```bash
curl -X POST "http://localhost:8000/detect" -F "file=@ruta/a/tu/foto.jpg"
```

Con categor√≠a y umbrales:

```bash
curl -X POST "http://localhost:8000/detect?category=lunch&box_threshold=0.35" -F "file=@plato.jpg"
```

## Docker

```bash
docker build -t nutri-ai-backend .
docker run -p 7860:7860 nutri-ai-backend
```

API en **http://localhost:7860** (docs en http://localhost:7860/docs).

## Despliegue en Hugging Face Spaces

1. Crea un nuevo Space con SDK **Docker**.
2. Sube `main.py`, `detection/`, `requirements.txt`, `Dockerfile`, `README.md`.
3. El Space construir√° la imagen y expondr√° la API en el puerto 7860.
4. Opcional: en **Settings ‚Üí Variables and secrets** puedes definir `GROUNDING_DINO_MODEL_ID`, `GROUNDING_DINO_BOX_THRESHOLD`, `GROUNDING_DINO_TEXT_THRESHOLD`.

## MLOps: guardar correcciones en Supabase

Si configuras **SUPABASE_URL** y **SUPABASE_SERVICE_ROLE_KEY** en el entorno (o en un `.env` dentro de `nutri-ai-backend/`), las correcciones del endpoint **POST /corrections** se guardan en Supabase en lugar del disco local:

- **Storage**: bucket `mlops-corrections` (crear en Dashboard ‚Üí Storage; nombre configurable con `MLOPS_BUCKET`).
- **Tabla**: `ingredient_corrections` (ejecutar la migraci√≥n `supabase/supabase-migration-mlops-corrections.sql` en el SQL Editor de Supabase).

Variables de entorno:

| Variable | Descripci√≥n |
|----------|-------------|
| `SUPABASE_URL` | URL del proyecto (ej. `https://xxx.supabase.co`) |
| `SUPABASE_SERVICE_ROLE_KEY` | Service role key (Dashboard ‚Üí Settings ‚Üí API) |
| `MLOPS_BUCKET` | Nombre del bucket de Storage (default: `mlops-corrections`) |

Si no defines estas variables, las correcciones se siguen guardando en `data/corrections/` (local).
